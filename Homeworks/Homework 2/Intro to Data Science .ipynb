{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"ITU-Lahore-Logo.jpeg\">\n",
    "<h1><center>Data Science Lab</center>\n",
    "<center>Information Technology University, Punjab</center>\n",
    "<center>Homework 2</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-**\n",
    "<h3>Instructions:</h3>\n",
    "* The aim of this homework is to give you understanding of Data Science pipeline. \n",
    "* You need to submit a detailed notebook with suitable headings and comments. Do not submit an erroneous notebook.\n",
    "* **Please complete this assignment and submit the complete folder with it's name as your registration number i.e. \"MSDS18###.zip\" on Google Classroom.**\n",
    "* <font color='red'>This is a well drafted assignment, I hope that there would be no need for you to look for solutions on internet. Simply follow the instructions and you would be good to go.</font>\n",
    "**-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is a commodity, but without ways to process it, its value is questionable. Data science is a multidisciplinary field whose goal is to extract value from data in all its forms. This discussion aims to explore the field of data science through data and its structure as well as the high-level process that you can use to transform data into value.\n",
    "\n",
    "Data science is a process. That's not to say it's mechanical and void of creativity. But, when you dig into the stages of processing data, from munging data sources and data cleansing to machine learning and eventually visualization, you see that unique steps are involved in transforming raw data into insight.\n",
    "\n",
    "The steps that you use can also vary (see Figure 1). In exploratory data analysis, you might have a cleansed data set that's ready to import into Python, and you visualize your result but don't deploy the model in a production environment. In another environment, you might be dealing with real-world data and require a process of data merging and cleansing in addition to data scaling and preparation before you can train your machine learning model.\n",
    "\n",
    "<font color='blue'>Figure 1. The Data Science Pipeline</font>\n",
    "<img src=\"files/Figure01.png\">\n",
    "\n",
    "Let's start by digging into the elements of the data science pipeline to understand the process.\n",
    "\n",
    "<h3>Data and its Structure</h3>\n",
    "\n",
    "Data comes in many forms, but at a high level, it falls into three categories: structured, semi-structured, and unstructured (see Figure 2). Structured data is highly organized data that exists within a repository such as a database (or a comma-separated values [CSV] file). The data is easily accessible, and the format of the data makes it appropriate for queries and computation (by using languages such as Structured Query Language (SQL) or Apache™ Hive™). Unstructured data lacks any content structure at all (for example, an audio stream or natural language text). In the middle is semi-structure data, which can include metadata or data that can be more easily processed than unstructured data by using semantic tagging. This data is not fully structured because the lowest-level contents might still represent data that requires some processing to be useful.\n",
    "\n",
    "Figure 2. Models of data\n",
    "<img src=\"files/Figure02.png\">\n",
    "\n",
    "Structured data is the most useful form of data because it can be immediately manipulated. The rule-of-thumb is that structured data represents only 20% of total data. Most of the data in the world (80% of available data) is unstructured or semi-structured.\n",
    "\n",
    "Note that much of what is defined as unstructured data actually has structure (such as a document that has metadata and tags for the content), but the content itself lacks structure and is not immediately usable. Therefore, it is considered unstructured.\n",
    "\n",
    "<h3>Data Engineering</h3>\n",
    "\n",
    "A survey in 2016 found that data scientists spend 80% of their time collecting, cleaning, and preparing data for use in machine learning. The remaining 20% they spend mining or modeling data by using machine learning algorithms. Although it's the least enjoyable part of the process, this data engineering is important and has ramifications for the quality of the results from the machine learning phase.\n",
    "\n",
    "I split data engineering into three parts: wrangling, cleansing, and preparation. Given the drudgery that is involved in this phase, some call this process data munging.\n",
    "\n",
    "<h3>Data Wrangling</h3>\n",
    "\n",
    "Data wrangling, simply defined, is the process of manipulating raw data to make it useful for data analytics or to train a machine learning model. This part of data engineering can include sourcing the data from one or more data sets (in addition to reducing the set to the required data), normalizing the data so that data merged from multiple data sets is consistent, and parsing data into some structure or storage for further use. Consider a public data set from a federal open data website. This data might exist as a spreadsheet file that you would need to export into a format more acceptable to data science languages (CSV or JavaScript Object Notation). The data source might also be a website from which an automated tool scraped the data. Finally, the data could come from multiple sources, which requires that you choose a common format for the resulting data set.\n",
    "\n",
    "This resulting data set would likely require post-processing to support its import into an analytics application (such as the Anaconda Enviornment, the GNU Data Language, or Apache Hadoop). Data wrangling, then, is the process by which you identify, collect, merge, and preprocess one or more data sets in preparation for data cleansing.\n",
    "\n",
    "<h3>Data cleansing</h3>\n",
    "\n",
    "After you have collected and merged your data set, the next step is cleansing. Data sets in the wild are typically messy and infected with any number of common issues, including missing values (or too many values), bad or incorrect delimiters (which segregate the data), inconsistent records, or insufficient parameters. In some cases, the data cannot be repaired and so must be removed; in other cases, it can be manually or automatically corrected.\n",
    "\n",
    "When your data set is syntactically correct, the next step is to ensure that it is semantically correct. In a data set that contains numerical data, you'll have outliers that require closer inspection. You can discover these outliers through statistical analysis, looking at the mean and averages as well as the standard deviation. Searching for outliers is a secondary method of cleansing to ensure that the data is uniform and accurate.\n",
    "\n",
    "<h3>Data preparation</h3>\n",
    "\n",
    "The final step in data engineering is data preparation (or preprocessing). This step assumes that you have a cleansed data set that might not be ready for processing by a machine learning algorithm. Here are a couple of examples where this preparation could apply.\n",
    "\n",
    "In some cases, normalization of data can be useful. Using normalization, you transform an input feature to distribute the data evenly into an acceptable range for the machine learning algorithm. This task can be as simple as linear scaling (from an arbitrary range given a domain minimum and maximum from -1.0 to 1.0). You can also apply more complicated statistical approaches. Data normalization can help you avoid getting stuck in a local optima during the training process (in the context of neural networks).\n",
    "\n",
    "Another useful technique in data preparation is the conversion of categorical data into numerical values. Consider a data set that includes a set of symbols that represent a feature (such as {T0..T5}). As a string, this isn't useful as an input to a neural network, but you can transform it by using a one-of-K scheme (also known as one-hot encoding).\n",
    "\n",
    "In this scheme (illustrated in Figure 3), you identify the number of symbols for the feature — in this case, six — and then create six features to represent the original field. For each symbol, you set just one feature, which allows a proper representation of the distinct elements of the symbol. You pay the price in increased dimensionality, but in doing so, you provide a feature vector that works better for machine learning algorithms.\n",
    "\n",
    "Figure 3. Transforming a string into a one-hot vector\n",
    "<img src=\"files/Figure03.png\">\n",
    "An alternative is integer encoding (where T0 could be value 0, T1 value 1, and so on), but this approach can introduce problems in representation. For example, in a real-valued output, what does 0.5 represent?\n",
    "\n",
    "<h3>Machine learning</h3>\n",
    "\n",
    "In this phase, you create and validate a machine learning model. Sometimes, the machine learning model is the product, which is deployed in the context of an application to provide some capability (such as classification or prediction). In other cases, the machine learning algorithm is just a means to an end. In these cases, the product isn't the trained machine learning algorithm but rather the data that it produces.\n",
    "\n",
    "This section discusses the construction and validation of a machine learning model. You can learn more about machine learning from data in Gaining invaluable insight from clean data sets.\n",
    "\n",
    "<h3>Model learning</h3>\n",
    "\n",
    "The meat of the data science pipeline is the data processing step. In one model, the algorithm can process the data, with a new data product as the result. But, in a production sense, the machine learning model is the product itself, deployed to provide insight or add value (such as the deployment of a neural network to provide prediction capabilities for an insurance market).\n",
    "\n",
    "Machine learning approaches are vast and varied, as shown in Figure 4. This small list of machine learning algorithms (segregated by learning model) illustrates the richness of the capabilities that are provided through machine learning.\n",
    "\n",
    "Figure 4. Machine learning approaches\n",
    "<img src=\"files/Figure04.png\">\n",
    "\n",
    "Supervised learning, as the name suggests, is driven by a critic that provides the means to alter the model based on its result. Given a data set with a class (that is, a dependent variable), the algorithm is trained to produce the correct class and alter the model when it fails to do so. The model is trained until it reaches some level of accuracy, at which point you could deploy it to provide prediction for unseen data.\n",
    "\n",
    "In contrast, unsupervised learning has no class; instead, it inspects the data and groups it based on some structure that is hidden within the data. You could apply these types of algorithms in recommendation systems by grouping customers based on the viewing or purchasing history.\n",
    "\n",
    "Finally, reinforcement learning is a semi-supervised learning algorithm that provides a reward after the model makes some number of decisions that lead to a satisfactory result. This type of model is used to create agents that act rationally in some state/action space (such as a poker-playing agent).\n",
    "\n",
    "We would limit our scope to the above points and skip Model Validation and Model Deployment for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please read the above Text and Answer the Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please edit this cell and paste your answer here:**\n",
    "\n",
    "Example:\n",
    "Q 20) A\n",
    "\n",
    "Q 1) \n",
    "\n",
    "Q 2) \n",
    "\n",
    "Q 3) \n",
    "\n",
    "Q 4) \n",
    "\n",
    "Q 5) \n",
    "\n",
    "Q 6) \n",
    "\n",
    "Q 7) \n",
    "\n",
    "Q 8) \n",
    "\n",
    "Q 9) \n",
    "\n",
    "Q 10) \n",
    "\n",
    "Q 11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 1) CSV file is an example of which type of data:\n",
    "> a) Structured Data\n",
    "<br>\n",
    "> b) Semi-Structured Data\n",
    "<br>\n",
    "> c) Unstructured Data\n",
    "<br>\n",
    "> d) Relational Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 2) Which of the following data can be queried using SQL or Hive.\n",
    "> a) Structured Data\n",
    "<br>\n",
    "> b) Semi-Structured Data\n",
    "<br>\n",
    "> c) Unstructured Data\n",
    "<br>\n",
    "> d) None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 3) Semantic tagging is a technique used to easily process which data\n",
    "> a) Structured Data\n",
    "<br>\n",
    "> b) Semi-Structured Data\n",
    "<br>\n",
    "> c) Unstructured Data\n",
    "<br>\n",
    "> d) Relational Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 4) According to a survey in 2016, data scientists spend 20% of their time\n",
    "> a) Collecting Data\n",
    "<br>\n",
    "> b) Cleaning Data\n",
    "<br>\n",
    "> c) Preparing Data\n",
    "<br>\n",
    "> d) Modelling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5) Process of manipulating raw data to make it useful for data analytics or to train a machine learning model is called\n",
    "> a) Data Wrangling\n",
    "<br>\n",
    "> b) Data Cleansing\n",
    "<br>\n",
    "> c) Data Preparation\n",
    "<br>\n",
    "> d) Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6) Which of the following are data cleaning issues?\n",
    "> a) Missing Values\n",
    "<br>\n",
    "> b) Incorrect Delimiters\n",
    "<br>\n",
    "> c) Inconsistent Records\n",
    "<br>\n",
    "> d) Outliers\n",
    "<br>\n",
    "> e) All of the mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q7) Which of the following is a way to detect outliers.\n",
    "\n",
    "> a) Looking at mean and standard deviation\n",
    "<br>\n",
    "> b) Looking at distribution of data\n",
    "<br>\n",
    "> c) Plotting using Box Plot\n",
    "<br>\n",
    "> d) All of the mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 8) Transforming an input feature to distribute the data evenly into an acceptable range is called\n",
    "> a) Normalization\n",
    "<br>\n",
    "> b) Modelling\n",
    "<br>\n",
    "> c) Standardization\n",
    "<br>\n",
    "> d) One-Hot Encoding\n",
    "<br>\n",
    "> e) None of the mentioned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 9) Transforming an input feature so that, it have zero mean and unit variance.\n",
    "> a) Normalization\n",
    "<br>\n",
    "> b) Modelling\n",
    "<br>\n",
    "> c) Scaling\n",
    "<br>\n",
    "> d) Standardization\n",
    "<br>\n",
    "> e) One-Hot Encoding\n",
    "<br>\n",
    "> f) None of the mentioned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 10) Which of the following is a pre-processing technique that is useful to avoid getting stuck in local optima during the training phase(in context of neural network)?\n",
    "> a) Normalization\n",
    "<br>\n",
    "> b) Modelling\n",
    "<br>\n",
    "> c) Standardization\n",
    "<br>\n",
    "> d) One-Hot Encoding\n",
    "<br>\n",
    "> e) None of the mentioned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 11) Which of the following is a pre-processing technique that converts categorical data into numerical?\n",
    "> a) Normalization\n",
    "<br>\n",
    "> b) Modelling\n",
    "<br>\n",
    "> c) Scaling\n",
    "<br>\n",
    "> d) Standardization\n",
    "<br>\n",
    "> e) One-Hot Encoding\n",
    "<br>\n",
    "> f) None of the mentioned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 12) In the section \"Machine Learning\", a machine learning model is said to have capability to classify and predict. Could you differentiate between classification and prediction with suitable examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Please write your answer here\n",
    "### Don't be a story teller\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q 13) Machine learning is majorly divided into three categories; supervised, unsupervised and reinforcement learning. Differentiate between these three with suitable examples? Please do not copy from the text provided, show your understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Please write your answer here\n",
    "### Don't be a story teller\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Titanic: Machine Learning From Disaster</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"files/titanic ship.jpg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been provided as a single training file:\n",
    "\n",
    "* training set (train.csv)\n",
    "\n",
    "The **training set** should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<h3>**Variable**</h3>|<h3>**Definition**</h3>|<h3>**Key**</h3>|\n",
    "|:-|:-|:-|\n",
    "|PassengerId|Unique Identifier of each passenger||\n",
    "|Survived|Survival|0 = No, 1 = Yes|\n",
    "|Pclass|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|Name|Name of Passenger||\n",
    "|Sex|Gender||\n",
    "|Age|Age in years||\n",
    "|SibSp|# of siblings / spouses aboard the Titanic||\n",
    "|Parch|# of parents / children aboard the Titanic||\n",
    "|Ticket|Ticket number||\n",
    "|Fare|Passenger fare||\n",
    "|Cabin|Cabin number||\n",
    "|Embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Detail of some of the variables is provided. You need to explore rest of the varibles yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Please import all the libraries in this block\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### \n",
    "### Using pandas library read the data in a dataframe\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### \n",
    "### Print Number of Rows and Columns in the dataset\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Print first 10 and last 10 rows of the data\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Print all the attribute/column names\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data types of Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Print datatypes of all attributes\n",
    "### Separate Discrete and Continuous Attributes programmatically\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Print the description of data\n",
    "### You should know what it means to describe a continuous attribute\n",
    "### And what it means to describe a discrete attribute\n",
    "### Make separate blocks for each of these\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### For this section you should plot 2 graphs for each attribute\n",
    "### First graph should display the distribution of data in each attribute\n",
    "### Second graph should display how the attribute changes with respect to target class\n",
    "###\n",
    "### Take advantage of matplotlib and any other plotting library\n",
    "### Remember your graph should be meaningful and properly labeled\n",
    "### You should understand that when the type of variable changes - it's representation also changes\n",
    "###\n",
    "### In some of the attributes you would discover that plotting them is not possible\n",
    "### Could you tell why plotting these attributes is not possible?\n",
    "### Is there any way to resolve this issue?\n",
    "###\n",
    "### Please remember not to make any changes in the original dataframe\n",
    "###\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Great! Let's Keep Going </h3>\n",
    "* I hope now you have a good understanding of data.\n",
    "* Let's move to the next phase of data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Check for duplicate rows. If found handle them.\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Check for missing values. If found handle them in the best possible manner\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Check for outliers. If found handle them in the best possible manner\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Check for any other data quality issues\n",
    "### Report these issues and also resolve them\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### \n",
    "### Do you think that all features are important in this scenario?\n",
    "###\n",
    "### Try to identify imoprtant features.\n",
    "### Give detail of the technique you apply.\n",
    "### You can search online for different ways.\n",
    "###\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Before you build your model\n",
    "### Please give a short description of attributes you wish to choose\n",
    "### Also state reason for choosing them\n",
    "###\n",
    "### Write your answer here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "* Decision Trees are very flexible, and expressive classifiers.\n",
    "* They can accommodate any kind of data i.e numeric, categorical and e.t.c.\n",
    "* They can accommodate any number of classes,they are inherently multi-class\n",
    "* Most importantly, they are **extremely interpretable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### \n",
    "### Before you train your model, split the data in training and testing set. \n",
    "### You would end up with 4 slices of data\n",
    "### X_train = contains training features\n",
    "### y_train = contains training labels\n",
    "### Y_train = contains testing features\n",
    "### y_test = contains testing labels\n",
    "###\n",
    "### Write your code here\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### \n",
    "### Using sklearn train a simple decision Tree Classifier on training data\n",
    "### Use default settings and do not change any parameters of the module\n",
    "###\n",
    "### Write your code here \n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Points To Ponder:\n",
    "### \n",
    "### Can you pass categorical(in string format) data to machine learning model.\n",
    "### If Yes. Then I definetly need to learn something from you.\n",
    "### If No. Explain that error and how you handled that error?\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Accuracy For Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Report your accuracy for training set and testing set both\n",
    "### i.e. \n",
    "### Training Accuracy = ???\n",
    "### Testing Accuracy = ???\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Simple Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starter to linear models, we will be using a simple linear model called **Logistic Regression.** \n",
    "* Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. \n",
    "* Logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
    "* This all might be very new to you. So for the time being, just consider **Logistic Regression as a linear classifier that classifies that a passenger survived or not survived.**\n",
    "* <font color='red'> Please do not complain if you haven't studied Logistic Regression. It's not a big deal, just Google it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### \n",
    "### Using sklearn\n",
    "### Train a simple Logistic Regression Model on training data\n",
    "### Use default settings and do not change any parameters of the module\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Accuracy For Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Report your accuracy for training set and testing set both\n",
    "### i.e. \n",
    "### Training Accuracy = ???\n",
    "### Testing Accuracy = ???\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Plot a graph that clearly shows the testing accuracy comparison of both models.\n",
    "### It should be properly labeled and intuitive.\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear participant, \n",
    "\n",
    "Please provide an honest and constructive feedback for the above test.\n",
    "\n",
    "Was it helpful?\n",
    "\n",
    "Was the time enough?\n",
    "\n",
    "Was it challenging?\n",
    "\n",
    "Did you learn something out of it?\n",
    "\n",
    "Did you had problem understanding the notebook?\n",
    "\n",
    "If there was something ambiguous. Please report.\n",
    "\n",
    "What improvements do you suggest?\n",
    "\n",
    "Any other comments you would like to add?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Please provide your feedback in here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This was an introductory notebook to real world data science problems.\n",
    "* We skipped some concepts but will go through them in future.\n",
    "* However, if you have reached so far. **Great Work!!!**\n",
    "<br><br>\n",
    "<img src=\"files/great-work.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://www.ibm.com/developerworks/library/ba-intro-data-science-1/index.html\n",
    "<br>\n",
    "[2] http://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "<br>\n",
    "[3] https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n",
    "<br>\n",
    "[4] Tan, Pang-Ning. Introduction to data mining. Pearson Education India, 2006.\n",
    "<br>\n",
    "[5] T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009.\n",
    "<br>\n",
    "[6] http://scikit-learn.org/stable/modules/tree.html\n",
    "<br>\n",
    "[7] J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.\n",
    "<br>\n",
    "[8] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.\n",
    "<br>\n",
    "[9] http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "<br>\n",
    "[10] http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "<br>\n",
    "[11] http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "<br>\n",
    "[12] https://matplotlib.org/contents.html\n",
    "<br>\n",
    "[13] https://docs.scipy.org/doc/numpy/reference/index.html\n",
    "<br>\n",
    "[14] http://pandas.pydata.org/pandas-docs/stable/\n",
    "<br>\n",
    "[15] http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been prepared by:<br><br>\n",
    "**Faizan Saeed**<br>\n",
    "**Graduate Fellow**<br>\n",
    "**Data Science Lab**<br><br>\n",
    "In case of any issue, please e-mail at: **MSDS17005@ITU.EDU.PK**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
